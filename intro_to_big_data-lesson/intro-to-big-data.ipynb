{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "\n",
    "# Intro to Big Data\n",
    "\n",
    "---\n",
    "\n",
    "![](https://snag.gy/SZOEv2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning Objectives\n",
    "*After this lesson, you will be able to:*\n",
    "- Recognize big data problems\n",
    "- Explain how the map reduce algorithm works\n",
    "- Understand the difference between high performance computing and cloud computing\n",
    "- Describe the divide and conquer strategy\n",
    "- Perform a map-reduce on a single node using python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "- [Introduction](#intro)\n",
    "- [What is big data?](#big-data)\n",
    "- [High performance computing (HPC)](#hpc)\n",
    "- [Cloud computing](#cloud)\n",
    "- [Parallelism](#parallelism)\n",
    "- [Divide and conquer](#dc)\n",
    "- [MapReduce](#mapreduce)\n",
    "- [MapReduce: key-value pairs](#kv-pairs)\n",
    "- [Combiners](#combiners)\n",
    "- [MapReduce in python](#python)\n",
    "    - [`mapper.py`](#mapper)\n",
    "    - [`reducer.py`](#reducer)\n",
    "    - [Running the code in terminal](#terminal)\n",
    "- [Independent practice](#ind-practice)\n",
    "- [Conclusion](#conclusion)\n",
    "- [Additional resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"intro\"></a>\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "This lesson identifies some major trends in the field of \"big data\" and data infrastructure, including common tools and problems that you may encounter working as a data scientist. \n",
    "\n",
    "It is time to take the tools you've learned to a new level by scaling up the size of datasets you can tackle!\n",
    "\n",
    "\n",
    "<img src=\"https://snag.gy/mDzP4d.jpg\" style=\"height: 300px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What do you think Big Data is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> **Big data is a hot topic nowadays. It refers to techniques and tools that allow to store, process and analyze large-scale (multi-terabyte) datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can you think of any datasets that would be \"big data\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For ex:\n",
    "\n",
    "- Facebook social graph\n",
    "- Netflix movie preferences\n",
    "- Large recommender systems\n",
    "- Activity of visitors to a website\n",
    "- Customer activity in a retail store (ie: Target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What challenges exist with such large amounts of data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Processing time\n",
    "- Cost\n",
    "- Architecture maintenance and setup\n",
    "- Hard to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"big-data\"></a>\n",
    "## What is \"big data\"?\n",
    "---\n",
    "\n",
    "Big data is a term used for data that exceeds the processing capacity of typical databases. We need a big data analytics team when the data is enormous and growing quickly but we need to uncover hidden patterns, unknown correlations, and build models. \n",
    "\n",
    "**There are three main features in big data (the 3 \"V\"s):**\n",
    "- **Volume**: Large amounts of data\n",
    "- **Variety**: Different types of structured, unstructured, and multi-structured data\n",
    "- **Velocity**: Needs to be analyzed quickly\n",
    "\n",
    "**4th V (unofficial big data tenet):**\n",
    "- **Value**: It's important to assess the value of predictions to business value.  Understanding the underpinnings of cost vs benefit is even more essential in the context of big data.  It's easy to misundersatnd the 3 V's without looking at the bigger picture, connecting the value of the business cases involved.\n",
    "\n",
    "![3v](./assets/images/3vbigdata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='hpc'></a>\n",
    "## High performance computing (HPC)\n",
    "---\n",
    "\n",
    "Supercomputers are very expensive, very powerful calculators used by researchers to solve complicated math problems.\n",
    "\n",
    "![supercomputer](./assets/images/supercomputer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can you think of advantages and disadvantages of HPC configurations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**PROS:**\n",
    "- can perform very complex calculation\n",
    "- centrally controlled\n",
    "- useful for research and defense complicated math problems\n",
    "\n",
    "**CONS:**\n",
    "- expensive\n",
    "- difficult to maintain (self-manged or managed hosting both incur operations overhead)\n",
    "- scalability is bounded (pre-bigdata era:  this would be medium data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='cloud'></a>\n",
    "## Cloud computing\n",
    "---\n",
    "\n",
    "Instead of one huge machine, what if we bought a bunch of (commodity) machines?\n",
    "\n",
    "> *Note: Comodity hardware is a term we used in operations to describe mixed server hardware but it can also refer to basic machines that you would use in an office setting as well.*\n",
    "\n",
    "![commodity hardware](https://snag.gy/fNYgt0.jpg)<center>*Actual AWS Datacenter*</center>\n",
    "\n",
    "**Can you think of advantages and disadvantages of this configuration?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**PROS:**\n",
    "- Relatively cheaper\n",
    "- Easier to maintain (as a user of the cloud system)\n",
    "- Scalability is unbounded (just add more nodes to the cluster)\n",
    "- Variety of turn-key solutions available through cloud providers\n",
    "\n",
    "**CONS:**\n",
    "- Complex infrastructure \n",
    "- Subject matter expertise required to leverage lower-level resources within infrastructure\n",
    "- Mainly tailored for parallelizable problems\n",
    "- Relatively small cpu power at the lowest level\n",
    "- More I/O between machines\n",
    "\n",
    "The term Big Data refers to the cloud computing case, where commodity hardware with unlimited scalability is used to solve highly parallelizable problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do you think many computers process data?\n",
    "\n",
    "**How does this contrast to how you perform analysis on your laptop?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='parallelism'></a>\n",
    "## Parallelism\n",
    "---\n",
    "\n",
    "The conceptual foundation of Big Data processing is the idea that a problem can be computed by multiple machines in pieces simultaneously. This is many resources being used in \"parallel\".\n",
    "\n",
    "![](https://snag.gy/MknIN6.jpg)\n",
    "\n",
    "- Running multiple instances to process data\n",
    "- Data can be subset and solved iteratively \n",
    "- Sub-solutions can be solved independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='dc'></a>\n",
    "## Divide and conquer\n",
    "---\n",
    "\n",
    "<img src=\"https://snag.gy/xh2mJA.jpg\">\n",
    "\n",
    "The \"Divide and Conquer\" strategy is a fundamental algorithmic technique for solving a task. The steps are:\n",
    "\n",
    "1. Split the task into subtasks\n",
    "- Solve these subtasks independently\n",
    "- Recombine the subtask results into a final result\n",
    "\n",
    "For a problem to be suitable for the divide and conquer approach it must be able to be broken into smaller independent subtasks. Many processes are suitable for this strategy, but there are plenty that do not meet this criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='mapreduce'></a>\n",
    "## MapReduce\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://snag.gy/XBgCOs.jpg\">\n",
    "\n",
    "The term **Map Reduce** indicate a two-phase divide and conquer algorithm initially invented and publicized by Google in 2004. It involves splitting a problem into subtasks and processing these subtasks in parallel. It consists of two phases:\n",
    "\n",
    "1. The **mapper** phase\n",
    "- The **reducer** phase\n",
    "\n",
    "In the **mapper phase**, data is split into chunks and the same computation is performed on each chunk, while in the **reducer phase**, data is aggregated back to produce a final result.\n",
    "\n",
    "Map-reduce uses a functional programming paradigm.  The data processing primitives are mappers and reducers.\n",
    "\n",
    "- **Mappers** – filter & transform data\n",
    "- **Reducers** – aggregate results\n",
    "\n",
    "The functional paradigm is good for describing how to solve a problem, but not very good at describing data manipulations (eg, relational joins)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='kv-pairs'></a>\n",
    "## MapReduce: key-value pairs\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://snag.gy/k2FCar.jpg\">\n",
    "\n",
    "Data is passed through the various phases of a **map-reduce pipeline** as key-value pairs.\n",
    "\n",
    "**What python data structures could be used to implement a key value pair?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- **Dictionary**\n",
    "- **Tuple** of 2 elements\n",
    "- **List** of 2 elements\n",
    "- Named **tuple**\n",
    "\n",
    "To understand map reduce you need to always keep in mind that data is flowing through a pipeline as key-value pairs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='combiners'></a>\n",
    "## Combiners\n",
    "---\n",
    "\n",
    "Combiners are intermediate reducers that are performed at node level in a multi-node architecture.\n",
    "\n",
    "![](https://snag.gy/lFYfoC.jpg)\n",
    "\n",
    "When data is really large we can distribute it to several mappers running on different machines. Sending a long list of `(word, 1)` pairs to the reducer node is not efficient. We can first aggregate at mapper node level and send the result of the aggregation to the reducer. This is possible because aggregations are associative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"python\"></a>\n",
    "## MapReduce in python\n",
    "---\n",
    "\n",
    "Let's perform map reduce in python. Below you can find the code for a simple mapper and reducer that calculate the word count on a text.\n",
    "\n",
    "Let's look at them in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='mapper'></a>\n",
    "### `mapper.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# mapper.py\n",
    "import sys\n",
    "\n",
    "# get text from standard input\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Check:** What kind of input does `mapper.py` expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**AND** what kind of output does `mapper.py` produce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='reducer'></a>\n",
    "### `reducer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\t0\n"
     ]
    }
   ],
   "source": [
    "# reducer.py\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word, count = line.split('\\t', 1)\n",
    "    \n",
    "    # try to count, if error continue\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Check:** what kind of input does `reducer.py` expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Check:** what kind of output does `reducer.py` produce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='terminal'></a>\n",
    "### Running the code in terminal\n",
    "\n",
    "**You can find `mapper.py`, `reducer.py` and some text input files in the `code` directory.**\n",
    "\n",
    "The code can be run with the following command from terminal:\n",
    "\n",
    "```bash\n",
    "cat <input-file> | python mapper.py | sort -k1,1 | python reducer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Check:** can you explain what each of the 4 steps in the pipe does?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- cat: read the file and streams it line by line\n",
    "- mapper\n",
    "- sort: shuffles the mapper output to sort it by key so that counting is easier\n",
    "- reducer: aggregates by word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Check:** can you find how our previous example *could* be represented in the diagram below?\n",
    "![map reduce word count](./assets/images/word_count_dataflow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Independent practice\n",
    "---\n",
    "\n",
    "Now that you have a basic word counter set up in python, try doing some of the following:\n",
    "\n",
    "1. Process a much larger text file (find one of your choice on the internet)\n",
    "    - For example,  a page from wikipedia or a blog article. If you're really ambitious you can take books from project gutemberg.\n",
    "- Try to see how the execution time scales with file size.\n",
    "- Read [this article](http://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html) for some very powerful shell tricks.  Learning to use the shell will save you tons of time munging data on your filesystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion\n",
    "---\n",
    "\n",
    "We have learned about Big Data and the MapReduce process. MapReduce is an algorithm that works really well for aggregations on very large datasets.\n",
    "\n",
    "**Check:** now that you know how it works can you think of some more specific business applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Examples:**\n",
    "\n",
    "- process log files to find security breaches\n",
    "- process medical records to assess spending\n",
    "- process news articles to decide on investments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='resources'></a>\n",
    "### Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "- [Top 500 Supercomputers](http://www.top500.org/lists/)\n",
    "- [Google Map Reduce paper](http://research.google.com/archive/mapreduce.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
